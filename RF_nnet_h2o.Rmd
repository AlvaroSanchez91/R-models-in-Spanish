---
title: Machine Learning II \newline Trabajo de evaluación de los temas 1,2, 6 y 7
author: "Álvaro Sánchez Castañeda"
date: "22 de mayo de 2017"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Ejercicio 1:
###Cargar el data frame “LetterRecognition” de la libería mlbench, que contiene datos apropiado para construir un sistema de reconocimiento de caracteres. La variable lettr es de tipo factor, presentando 26 niveles, cada uno es una letra mayúscula. Establecer la semilla del generador de números pseudo-aleatorios de R mediante set.seed(m), siendo m el número obtenido con las tres últimas cifras del DNI, y elegir aleatoriamente dos letras. Utilizando los casos que correspondan a alguna de ambas letras, construir de forma razonada y comparar modelos de clasificación binaria basados en Random Forests y el Perceptrón Multicapas (nnet).

## Lectura de datos y librerias.

Carguemos las librerias que se han usado para este ejercicio.

```{r, message=FALSE, warning=FALSE}
library(mlbench)#datos
library(rpart)#arboles
library(rpart.plot)
library(randomForest)#random forest
library(caret)#optimizacion parametros
library(ROCR)#AUC y ROC
library(nnet)#redes neuronales
library(h2o)#deep learning
```

Lectura de datos.
```{r, message=FALSE, warning=FALSE}
data("LetterRecognition")
datos=LetterRecognition
rm(LetterRecognition)
head(datos)
```



Escojamos aleatoriamente dos caracteres.
```{r}
m=176
set.seed(m)
sub_lettr=sample(levels(datos$lettr),2)
sub_lettr
```

Procedamos a seleccionar los datos correspondientes a B y H.
```{r}
sub_datos=datos[datos$lettr %in% sub_lettr,]
table(sub_datos$lettr)
sub_datos=droplevels(sub_datos)#eliminamos las letras que no nos interesan
table(sub_datos$lettr)
```

## Partición train test.

```{r}
set.seed(123)
ind=1:nrow(sub_datos)
ind_test=sample(ind,floor(nrow(sub_datos)*0.25))
train=sub_datos[-ind_test,]
test=sub_datos[ind_test,]
```



## Función para evaluar modelos.
 Podemos definir una función que evalue con distintas medidas de ajuste los modelos sobre el conjunto test. Tenemos solo dos caracteres a clasificar, de modo que si interpretamos estos valores como positivo y negativo, podemos calcular la curva ROC y la medida AUC. 
 
```{r}
evaluacion=function(modelo,test=test,title='modelo', plot_cor=T , nnet_model=F){
  predicciones=predict(modelo,test,type = 'class')
  matriz_confusion=table(test$lettr,predicciones)
  accuracy=100*sum(diag(prop.table(matriz_confusion)))
  true_B_ratio=100*diag(prop.table(matriz_confusion, 1))[1]
  true_H_ratio=100*diag(prop.table(matriz_confusion, 1))[2]
  if (nnet_model){
  probabi=predict(modelo,test)  
  }
  else{
  probabi=predict(modelo,test,type = 'prob')[,2]#interpretamos H como yes
  }
  prediobj<-prediction(probabi,test$lettr)
  if(plot_cor){
  plot(performance(prediobj, "tpr","fpr"),
       main=paste("CURVA COR ",title),ylab='true_H_ratio',xlab='1 - true_B_ratio')
    abline(a=0,b=1,col="blue",lty=2)
  print(prop.table(matriz_confusion))
  }
  
  auc<- as.numeric(performance(prediobj,"auc")@y.values)
  medidas=cbind(accuracy,true_B_ratio,true_H_ratio,auc)
  row.names(medidas)=title
  return(medidas)
}
```



## Arbloes de clasificación.

Antes de construir bosques aleatoriós, podemos construir arboles de clasificación y comparara los modelos.

### Arbol completo.

```{r}
arbol_comleto=rpart(lettr~., data = train,cp=0.00)
evaluaciones=evaluacion(arbol_comleto,test,'arbol_comleto')
evaluaciones
```



### Arbol recortado.

Escojamos el parametro de complejidad cp mediante la regla 1-ES, y comparemos los resultados con el arbol recortado.

```{r}
plotcp(arbol_comleto)
cptabla<- arbol_comleto$cptable
CP1ES<- min(cptabla[,4])+cptabla[which.min(cptabla[,4]),5] 
cprecorte<- cptabla[cptabla[,4]<CP1ES,][1]
arbol_recortado=prune(arbol_comleto,cprecorte)
evaluaciones=rbind(evaluaciones,evaluacion(arbol_recortado,test,'arbol_recortado',F))
evaluaciones
```

Los resultados son peores (esto se debe a que en la regla 1-ES consideramos las varianzas de los errores de validación cruzada). Construyamos un arbol con la libreria caret.



### Arbol con caret.

```{r,fig.height=3.5}
grid <- expand.grid(cp=as.vector( arbol_comleto$cptable[,1]))#usamos la rejilla de rpart
trctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 2, allowParallel = T)
set.seed(3333)
arbol_caret <- train(lettr ~., data = train, method = "rpart",
                   trControl = trctrl,tuneGrid=grid)
plot(arbol_caret)
arbol_caret$bestTune
evaluaciones=rbind(evaluaciones,evaluacion(arbol_caret$finalModel,test,'arbol_caret',F))
evaluaciones
```
 Mediante dos repeticiones de una validación cruzada, parece que el mejor parametro de complejidad es ligeramente mayor que cero. En cambio, al evaluar sobre el conjunto test parece que es mejor tomar cp igual a cero. Tenemos unos resultados excepcionalmente buenos, pero veamos si es posible mejorarlos algo mediante bosques aleatorios.

## Bosques aleatórios.

### Bosque aleatorio con parametros por defecto.
 Comencemos construyendo un modelo con los parametros por defecto que proporciona el paquete randomForest.
 
```{r , fig.height=3.5}
RF<- randomForest(lettr ~ ., data=train,
                  importance=TRUE,do.trace=F)
plot(RF)
legend("topright",col=1:3,lty=1:3,
       legend=c("OOB",levels(train$lettr)))
evaluaciones=rbind(evaluaciones, evaluacion(RF,test,'RF',F))
evaluaciones
```
Mejoramos los resultados (curiosamente, el ratio de B verdaderos es igual al anterior).  Parece que con menos arboles obtendríamos resultados similares.



### Medidas de importancia.
 Los bosques aleatorios dan diversas medidas de importancia para las variables, veamoslas.

```{r}
varImpPlot(RF)
tabla_impor=importance(RF)
tabla_impor[order(-tabla_impor[,1]),]
```



### Bosque aleatorio con selección de variables.
Basandonos en las importancias anteriores, podemos tratar de seleccionar variables para ver si obtenemos mejores resultados.

```{r}
set.seed(123)
eliminacion=row.names(tabla_impor[order(tabla_impor[,1]),])[1:7]#eliminamos variables
RF_2<- randomForest(lettr ~ ., data=train[,! names(train)  %in% eliminacion],
                  importance=TRUE,do.trace=F)
evaluaciones=rbind(evaluaciones, evaluacion(RF_2,test,'RF_2',F))
evaluaciones
```

### Optimización de parametros con tuneRF.
 Parece que la selección de variables da buenos resultados y acertamos todos los elementos de la clase B. Otra opción para tratar de mejorar el modelo, es escoger el parámetro mtry (controla el número de variables que aleatoriamente se toman para construir los arboles).
 
```{r, echo=TRUE, fig.height=3}
tune=tuneRF(train[,-1],train[,1],trace = F,doBest=FALSE)#separamos la variable respuesta
mtry_tune=tune[which(tune[,2]==min(tune[,2])),1]#tomamos el parametro que da menor error OOB
```

```{r, echo=TRUE, fig.height=3}
RF_3<- randomForest(lettr ~ ., data=train[,! names(train)  %in% eliminacion],
                    importance=TRUE,do.trace=F,mtry=mtry_tune)
evaluaciones=rbind(evaluaciones, evaluacion(RF_3,test,'RF_3',F))
print(evaluaciones)
```

Apenas hay diferencia entre los dos últimos modelos. Podemos tratar de usar caret para optimizar mtry.

### Optimización de parametros con caret.


```{r,fig.height=3, fig.height=3, warning=FALSE}
trctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 1, allowParallel = T)
set.seed(3333)
RF_caret <- train(lettr ~., data = train[,! names(train)  %in% eliminacion], 
                  method = "rf",trControl = trctrl,tuneLength = 8,ntree=300)
plot(RF_caret)
evaluaciones=rbind(evaluaciones, evaluacion(RF_caret$finalModel,test,'RF_caret',F))
evaluaciones
```



No mejoramos los resultados (son practicamente iguales). Pasemos a usar otro tipo de modelos, rededes de neuronas artificiales, y veamos si conseguimos unos resultados tan buenos como los anteriores.

## Redes neuronales.

Los anteriores modelos se basaban en arboles, por lo que tipificar los datos no afectaba demasiado a los resultados. En cambio, si que es recomendable para el uso de redes neuronales.

### Tipificación.
```{r}
preProcValues <- preProcess(train, method = c("center","scale"))#usamos caret
train_processed <- predict(preProcValues, train)
test_processed <- predict(preProcValues, test)
```

### Optimización de parametros.
Trabajando con redes pequeñas, se pueden optimizar valores de regularización así como el numero de nodos. Usaremos los conjuntos con y sin tipificar para comparar.

```{r, warning=FALSE}
ctrl <- trainControl(method="cv",classProbs=TRUE,
                     summaryFunction = twoClassSummary) 
modeloPM <- train(lettr ~ ., data = train, #Usa por defecto entropia para clasificacion
                  trace=F,
                  method = "nnet", 
                  trControl = ctrl, 
                  preProcess = c("center","scale"), 
                  tuneGrid = expand.grid(size=seq(1, 11, 2),decay=c(0,0.1)))
modeloPM$bestTune
evaluaciones=rbind(evaluaciones, evaluacion(modeloPM$finalModel,test,'nnet',F,nnet_model=T))

modeloPM_processed <- train(lettr ~ ., data = train_processed, 
                            trace=F,
                            method = "nnet", 
                            trControl = ctrl,  
                            tuneGrid = expand.grid(size=seq(1, 11, 2),decay=c(0,0.1)))
modeloPM_processed$bestTune
```
Parece que lo mejor es escoger 7 nodos con datos tipificados y 11 para datos sin tipificar, comparemos los modelos obtenidos sobre el conjunto test.



```{r, warning=FALSE}
evaluaciones=rbind(evaluaciones,evaluacion(modeloPM_processed$finalModel,
                                           test_processed,'nnet_scaled',F,nnet_model=T))
evaluaciones
```

 Tal como esperabamos, el modelo construido sobre datos sin tipificar es bastante peor. Veamos a continuación que ocurre si aumentamos considerablemente el numero de nodos.
 
```{r}
red<-  nnet(lettr~., data=train_processed,size=50,trace=F)
evaluaciones=rbind(evaluaciones,evaluacion(red,test_processed,'nnet_50',F,nnet_model=T))
evaluaciones
```

 Esto incita a tratar de construir redes aun mayores, del tipo "deep learning".
 

 
## Modelos deep learning.

La libreria h2o proporciona herramientas para construir este tipo de modelos.

### Funcion evaluación.

La función definida anteriormente no será compatible con los objetos proporcionados por la libreria h2o. Definnamos una nueva función que devuelva los datos con la misma estructura para poder comparar con los resultados anteriores. No usaremos los datos test en esta función, ya que los modelos de h2o perpiten calcular estas medidas sobre el conjunto test (solo ordenaremos dichas medidas).

```{r}
evaluacion_h2o=function(modelo,title='modelo'){
  cm=h2o.confusionMatrix(modelo,valid = T)
  accuracy=100*(1-cm$Error[3])
  true_B_ratio=100*(1-cm$Error[1])
  true_H_ratio=100*(1-cm$Error[2])
  auc<- h2o.auc(modelo,valid = T)
  medidas=cbind(accuracy,true_B_ratio,true_H_ratio,auc)
  row.names(medidas)=title
  return(medidas)
}
```


### Inicialización del servicio h2o.

```{r, eval=FALSE}
localH2O = h2o.init(nthreads = 5)
```

```{r, include=FALSE}
localH2O = h2o.init(nthreads = 5)#lo duplico, un chunk muestra codigo, el otro ejecuta
#esto lo hago por que no quiero mostrar la salida, pero si el codigo
```

### Conversión de los datos al formato h2o.

```{r, eval=FALSE}
train.hex <- as.h2o(train_processed)
test.hex<- as.h2o(test_processed)
```

```{r, include=FALSE}
train.hex <- as.h2o(train_processed)
test.hex<- as.h2o(test_processed)
```

### Modelo deep learning.
 Red con dos capas ocultas de doscientos nodos cada una.
 
 
```{r, eval=FALSE}
modelo_h2o <- h2o.deeplearning(
  x = 2:17, y = 1, 
  training_frame = train.hex,
  validation_frame=test.hex,
  distribution="multinomial",
  activation = 'RectifierWithDropout',
  hidden = c(200, 200),
  l1 = 1e-5,
  l2 = 1e-5,
  hidden_dropout_ratio = c(0.5, 0.5),
  input_dropout_ratio = 0.2,
  epochs = 50,
  rho = 0.99,
  epsilon = 1e-8,
  train_samples_per_iteration = 500)
```

```{r, include=FALSE}
modelo_h2o <- h2o.deeplearning(
  x = 2:17, y = 1, 
  training_frame = train.hex,
  validation_frame=test.hex,
  distribution="multinomial",
  activation = 'RectifierWithDropout',
  hidden = c(200, 200),
  l1 = 1e-5,
  l2 = 1e-5,
  hidden_dropout_ratio = c(0.5, 0.5),
  input_dropout_ratio = 0.2,
  epochs = 50,
  rho = 0.99,
  epsilon = 1e-8,
  train_samples_per_iteration = 500)
```

Comparemos este modelo con los construidos anteriormente

```{r}
evaluaciones=rbind(evaluaciones,evaluacion_h2o(modelo_h2o,'modelo_h2o'))
evaluaciones
```



### Grid Search.

Podemos buscar el mejor modelo atraves distintas combinaciones de coeficientes de regularización (L1) y configuraciones de capas ocultas (usando validación cruzada). Tomaremos redes de tamaño considerable, por lo que esto puede tardar bastante.



```{r, eval=FALSE}
hidden_search = list(c(100,100),c(500,500), c(500,500,500),c(200,200,200),c(100,100,100))
l1_search <- c(1e-4,1e-3)
hyper_params= list(hidden = hidden_search, l1 = l1_search)

modelo_h2o_grid = h2o.grid("deeplearning",
                           x = 2:17, y = 1, 
                           training_frame = train.hex,
                           validation_frame=test.hex,
                           distribution="multinomial",
                           activation = 'RectifierWithDropout',
                           hyper_params = hyper_params,
                           nfolds=5, #Par?metro de Validaci?n cruzada
                           score_interval = 2,
                           epochs = 50,
                           stopping_rounds = 3,
                           stopping_tolerance = 0.05,
                           stopping_metric = "misclassification")

```

```{r, include=FALSE}
hidden_search = list(c(100,100),c(500,500), c(500,500,500),c(200,200,200),c(100,100,100))
l1_search <- c(1e-4,1e-3)
hyper_params= list(hidden = hidden_search, l1 = l1_search)

modelo_h2o_grid = h2o.grid("deeplearning",
                           x = 2:17, y = 1, 
                           training_frame = train.hex,
                           validation_frame=test.hex,
                           distribution="multinomial",
                           activation = 'RectifierWithDropout',
                           hyper_params = hyper_params,
                           nfolds=5, #Par?metro de Validaci?n cruzada
                           score_interval = 2,
                           epochs = 50,
                           stopping_rounds = 3,
                           stopping_tolerance = 0.05,
                           stopping_metric = "misclassification")

```

Tomemos el mejor modelo.

```{r, eval=FALSE}
nmodelos=length(modelo_h2o_grid@model_ids)
ErrorVC=numeric(nmodelos)
for (i in 1:nmodelos)
{
  model_id= modelo_h2o_grid@model_ids[[i]]
  Entrop <- h2o.logloss(h2o.getModel(model_id), xval = TRUE)
  ErrorVC[i]=Entrop
}
nombre_modelo=modelo_h2o_grid@model_ids[[which.min(ErrorVC)]]
modelo_h2o_2=h2o.getModel(nombre_modelo)
```

```{r, include=FALSE}
nmodelos=length(modelo_h2o_grid@model_ids)
ErrorVC=numeric(nmodelos)
for (i in 1:nmodelos)
{
  model_id= modelo_h2o_grid@model_ids[[i]]
  Entrop <- h2o.logloss(h2o.getModel(model_id), xval = TRUE)
  ErrorVC[i]=Entrop
}
nombre_modelo=modelo_h2o_grid@model_ids[[which.min(ErrorVC)]]
modelo_h2o_2=h2o.getModel(nombre_modelo)
```



Veamos que configuración tiene.

```{r}
modelo_h2o_2@parameters$hidden
modelo_h2o_2@parameters$l1
```

Por ultimo, comparemoslo con el resto de modelos por medio del conjunto test.

```{r}
evaluaciones=rbind(evaluaciones,evaluacion_h2o(modelo_h2o_2,'modelo_h2o_2'))
evaluaciones
```

En general vemos que tenemos buenos resultados, pero parece que la mejor opción es un random forest con las variables seleccionadas. Tal vez podríamos usar dichas variables para una red neuronal.

### Deep Learning con seleccion de variables.

```{r, eval=FALSE}
#Seleccionamos variables
train_processed_select=train_processed[,! names(train_processed)  %in% eliminacion]
test_processed_select=test_processed[,! names(test_processed)  %in% eliminacion]

#Cambiamos el formato
train.hex_select <- as.h2o(train_processed_select)
test.hex_select<- as.h2o(test_processed_select)

#Entrenamos el modelo
modelo_h2o_sel <- h2o.deeplearning(
  x = 2:10, y = 1, 
  training_frame = train.hex_select,
  validation_frame=test.hex_select,
  distribution="multinomial",
  activation = 'RectifierWithDropout',
  hidden = c(200, 200,200),
  l1 = 1e-5,
  l2 = 1e-5,
  hidden_dropout_ratio = c(0.5, 0.5, 0.5),
  input_dropout_ratio = 0.2,
  epochs = 50,
  rho = 0.99,
  epsilon = 1e-8,
  train_samples_per_iteration = 500)
```

```{r, include=FALSE}
#Seleccionamos variables
train_processed_select=train_processed[,! names(train_processed)  %in% eliminacion]
test_processed_select=test_processed[,! names(test_processed)  %in% eliminacion]

#Cambiamos el formato
train.hex_select <- as.h2o(train_processed_select)
test.hex_select<- as.h2o(test_processed_select)

#Entrenamos el modelo
modelo_h2o_sel <- h2o.deeplearning(
  x = 2:10, y = 1, 
  training_frame = train.hex_select,
  validation_frame=test.hex_select,
  distribution="multinomial",
  activation = 'RectifierWithDropout',
  hidden = c(200, 200,200),
  l1 = 1e-5,
  l2 = 1e-5,
  hidden_dropout_ratio = c(0.5, 0.5, 0.5),
  input_dropout_ratio = 0.2,
  epochs = 50,
  rho = 0.99,
  epsilon = 1e-8,
  train_samples_per_iteration = 500)
```

Comparemos los resltados.

```{r}
evaluaciones=rbind(evaluaciones,evaluacion_h2o(modelo_h2o_sel,'modelo_h2o_sel'))
evaluaciones
```

No parece que merezca la pena usar esta selección.